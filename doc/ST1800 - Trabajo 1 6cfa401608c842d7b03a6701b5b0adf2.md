# ST1800 - Trabajo 1

## Introducción

### Integrantes

1. Emanuele Travanti - [etravanti@eafit.edu.co](mailto:etravanti@eafit.edu.co)
2. José Carlos Díaz -  [jcdiazm1@eafit.edu.co](mailto:jcdiazm1@eafit.edu.co)
3. Felipe Carrasquilla  - [fcarrasq@eafit.edu.co](mailto:fcarrasq@eafit.edu.co)
4. Carlos Tezna Sanz - [cteznas@eafit.edu.co](mailto:cteznas@eafit.edu.co)

### Objetivos específicos

1. Conocer en mayor profundidad las arquitecturas de una solución big data de un proveedor de nube, en este caso se seleccionará dentro del curso Amazon AWS, sin embargo, si el equipo desea utilizar y trabajar otra nube no hay problema, siempre y cuando tenga los accesos suficientes para desarrollar el caso de estudio.
2. Desarrollar un caso de estudio que aplique las primeras etapas del ciclo de vida de un proyecto de Big Data Analytics contemplando al menos los siguientes pasos:
    1. Identificación, descarga e ingesta de Fuentes de datos (datasets, APIs, BD, etc) de diferentes tipos.
    2. Diseñar un Data Lake como Almacenamiento de los datos de diferentes origines, tipos y estructuras

3. Adoptar una arquitectura de referencia para datalakes
4. definir las zonas que contendrá el datalake. definir la estructura de directorios más óptimo
5. Ingesta y Almacenamiento de datos en el Data Lake en la zona ‘raw’
6. Realizar uno (raw) y otro para (trusted) de catalogación de datos utilizando AWS Glue
7. Realizar uno o dos procesos ETL para preparar los datos hacia las zonas ‘trusted y ‘refined.
8. Realizar consultas básicas SQL mediante AWS Athena de los datos almacenados en el datalake mediante la catalogación realizada por AWS Glue.

## 1.  Definición del Datalake

Para el datalake se almacenaron las siguientes fuentes de información:

[Fuentes de información Datalake](https://www.notion.so/476da12802be4c5cb0a9738f3e4e612e)

**Nota**: 

En las fuentes donde se solicitaba tomar algún dataset, como medata, [datos.gov.co](http://datos.gov.co) y [data.gov](http://data.gov)  por lo que se emplearon los siguientes datasets:

[Fuentes descargadas](https://www.notion.so/b26e4523d331452fbf5968e57b62aa32)

Sin embargo existieron algunos Datasets que tuvieron un peso muy grande y no fue posible descargarse o realizar ingesta por limitaciones de conexión de internet

[Datasets que no fueron descargados*](https://www.notion.so/42048976bacd430891a4ca778ca8ec22)

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled.png)

***Nota: el dataset reviews and rating fue descargado parcialmente**

## Estructura del DataLake

El DataLake fue estructurado usando el servicio de Amazon S3 (Simple Storage Service) mediante una cuenta federada, suministrada por EAFIT.

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%201.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%201.png)

El ARN del datalake es `arn:aws:s3:::st1800datalake2021`  en US East (N virginia) us-east-1.

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%202.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%202.png)

### Zonas del datalake

El datalake `st1800datalake2021` fue definido empleando una convención de zonas, según la calidad de la información, en ellas se destacan las siguientes:

1. **01transient:** destinada a almacenar archivos compresos, información transitoria y tipo "papelera"
2. **02raw:** almacena los datos descompresos de las fuentes. Sin sufrir transformaciones más allá de un conversión entre formatos.  Principalmente compuesta por archivos no estructurados (texto) y semiestructurados (JSON, CSV, TSV...)
3. **03Trusted:**  almacena los datos que han sufrido almenos un proceso de transformación y están considerados como "aptos" para procesos de analítica.
4. **04Refined:** destinada para el almacenamiento de datasets (trusted) que han sufrido algún procesamiento de analítica y cuyo resultado es conveniente almacenar

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%203.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%203.png)

Estructura de zonas del Datalake en Amazon S3

### Estructura interna de las zonas

Las zonas del datalake, en particular la zona **03raw ,** están enfocadas al manejo de archivos según el tipo de información (genéricamente) luego según el sub-tipo y finalmente el propóstio. Esto significa que las estructura interna tiene un enfoque descriptivo en las carpetas.

La estructura de la carpeta 02raw, contiene los datasets listados previamente en las fuentes de información:

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%204.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%204.png)

Entrando a alguna de ellas, se puede apreciar la estructura de interna de cada una, por ejemplo la carpeta 01_COVID de la zona raw tiene, el subtipo 01_datos_Covid_colombia, que almacena a su vez los registros en formato semi-estructurado (csv).

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%205.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%205.png)

Subcarpeta de zona 02raw

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%206.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%206.png)

Carpeta con información específica

Es posible, que según la información contenida puedan existir sub-estructuras  en unas carpetas con mayor nivel que otra. Ejemplo, la carpeta 05_peliculas.

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%207.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%207.png)

Subcarpeta de 02RawPermisos

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%208.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%208.png)

Sub folder específico de varios niveles.

### Ingesta de datos

La ingesta de los datos fue realizada mediante el portal WEB y sus funcionalidades de subida y de  **drag'n drop.** Los datos fueron descargados localmente y luego subidos.

Particularmente el contenido de la carpeta 02raw se muestra a continuación:

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%209.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%209.png)

Estructura local de la carpeta 02raw

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2010.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2010.png)

Información de la carpeta local 02raw

La ingesta de la información futura sería realizada de la misma forma.

### Permisos

La estructura del datalake en S3 permite la administración de permisos para lectura, edición, listado y combinados hacia otras cuentas, roles y perfiles de AWS.  En particular la zona 02Trusted tiene permisos de lectura y listado para el usuario de aws `emontoya@eafit.edu.co`

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2011.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2011.png)

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2012.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2012.png)

Así como también la carpeta **03Trusted** permisos globales  de **lectura** para cualquier usuario con cuenta AWS. EL URI de esta es:

`s3://st1800datalake2021/03trusted/`

Unos pasos del proceso:

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2013.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2013.png)

Hacer pública la carpeta trusted

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2014.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2014.png)

Confirmación del proceso

## Creación de catalogos con AWS Glue

AWS Glue nos permite catalogar (crear schemas o esquemas) de la información contenida en algún almacenamiento de AWS. En particular permite crear tablas o schemas de una Database utilizando un rastreador o Crawler.

En el proceso es neceario crear una base de datos:

 

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2015.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2015.png)

Luego añadir una tabla mediante nuestro Crawler, el cual es automatizado e infiere el esquema de forma automática desde nuestro bucket/carpeta:

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2016.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2016.png)

Es necesario en la definición del crawler tener presente como mínimo:

- La ruta del directorio que contiene el archivo (preferiblemente folder)
- Que el rol que se use esté definido y tenga permisos.
- La database o base de datos donde será almacenado el schema.

Luego es necesario inciarlo. Este proceso demorará en función de la complejidad, tipo de estructura (o falta de esta) y el numero de registros

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2017.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2017.png)

Inicio de Crawler para inferencia de schema de la carpeta "clima" de nuestro Bucket

Finalmente, este reportará su ejecución y la creación de las tablas

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2018.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2018.png)

Finalización de crawler

Particularmente, el dataset empleado en esta ejecución de crawler contiene la información de estaciones climáticas  de IDEAM , encontradas en la plataforma **datos.gov.co**

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2019.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2019.png)

Este contiene en su primera fila los nombres de los campos y tiene un encoding UTF-8.

Consultando la tabla creada por el Crawler, se observa que este identificó  el dataset y su schema:

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2020.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2020.png)

Tabla generada por el Crawler 

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2021.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2021.png)

Schema generado por el Crawler del dataset

### Creación de catálogo para BD COVID

Una segunda creación de catálogo fue realizada, en este caso con la información de COVID dispuesta en el datalake.

Para la el Crawler selecciono la ruta de dónde voy a tomar los datos

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2022.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2022.png)

Creo el nombre de la base de datos nueva que voy a crear

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2023.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2023.png)

Una vez creado el Crawler lo corremos

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2024.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2024.png)

## Consultas sobre objetos S3 usando AWS Athena

A pesar que los datos se encuentran semi-estructurados es posible realizar consultas con el lenguaje SQL sobre ellos empleando el servicio de AWS Athena y el schema previamente creado con AWS Glue.

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2025.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2025.png)

### Consultas con información de estaciones IDEAM

En este servicio, en la parte izquierda ubicamos la database "ideam" y sobre el query editor ejecutamos la sentencia SQL

```powershell
select * from "ideam"."est_clima"
```

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2026.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2026.png)

Sentencia básica de exploración de registros con ATHENA

Esta operación generó los campos descritos en el schema en un tiempo menor a <1segundo.

Ahora bien, es de nuestro interés realizar dos consultas sobre este dataset:

1. **Determinar cuantas estaciones de clima hay por "tipo"**

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2027.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2027.png)

Query realizado

Con lo cual ATHENA resuelve:

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2028.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2028.png)

Resultado de ATHENA

2. **Determinar, por departamento, cuales son el top 10 que contienienen más estaciones suspendidas.**

Esto se puede realizar con el siguiente Query:

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2029.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2029.png)

Y el resultado de Athena:

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2030.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2030.png)

Donde el departamento de Antioquia tiene la mayor cantidad de estaciones suspendidas y Cauca el menor de ellas en este Top 10.

### Consultas para la información de reviews de IMDB

Sobre la información contenida  en las reviews de la información de IMDB,  se realizó el primer Query con el fin de determinar la duración promedio de las películas en función de su género.

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2031.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2031.png)

Resultado con AWS Athena:

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2032.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2032.png)

En este query observamos que El top de películas con mayor duración es Historia, con 117 minutos; seguido de romance con 115 y Bibliográficos con 110 minutos.

En una segunda sentencia, mostrada a continuación se quiere determinar y agrupar según las tablas de títulos y reviews, el promedio ponderado de voto según el género:

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2033.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2033.png)

El resultado de esta consulta es:

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2034.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2034.png)

En este caso el resultado muestra que el género documental tiene el más alto promedio ponderado de votos, con un promedio ponderado de 7.1  en su votación; seguido con música y biografía con 6.54 y 6.94  respectivamente.

## Flujo ETL empleando AWS Glue Studio

Es posible realizar un flujo simple de ETL para mover el dataset de nuestra zona 02Raw a la zona 03Refined. Esto se puede lograr mediante un proceso visual usando AWS Glue Estudio, el cual de forma transparente para el usuario, crea una instancia de Amazon EMR y realiza el proceso de transformación siguiendo un trabajo o Job.

### FLujo con información de estaciones IDEAM

En este caso nos interesa deshacernos de algunos campos de nuestro anterior Dataset y además filtrar las estaciones que NO están suspendidas, es decir que están "Activas" y almacenarlas en la zona **03Refined** y su folder **Clima**  

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2035.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2035.png)

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2036.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2036.png)

En el empleamos la herramienta visual para crear un trabajo con la plantilla de transformaciín de S3 a S3

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2037.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2037.png)

Definimos la fuente de nuestro dataset, en este caso un Data Catalog que creamos previamente

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2038.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2038.png)

Definimos algunos campos que no nos interesan en nuestro dataset limpio, como fecha_instalación y fecha de suspensión, y le hacemos "Drop"

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2039.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2039.png)

Al resultante, le hacemos un filtro, en donde los registros que nos interesan son las estaciones activas.

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2040.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2040.png)

Y definimos un formato de salida, y una ubicación:

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2041.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2041.png)

En este caso la ruta es `st1800datalake2021/03trusted/ideam`

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2042.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2042.png)

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2043.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2043.png)

Configuramos algunos parámertos del trabajo, colocándole el nombre del script:

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2044.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2044.png)

Configuramos el numero de trabajadores a un numero inferior, porque es un dataset pequeño y el numero de re intentos a 1 para evitar consumir créditos si la operación falla

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2045.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2045.png)

Guardamos y ejecutamos el trabajo

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2046.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2046.png)

Podemos monitorear nuestro trabajo y hacerle seguimiento al mismo entrando en el menú Monitoring.

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2047.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2047.png)

En el detalle del Job se encuentra más información, y luego de un tiempo se piede ver su estado, de terminado

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2048.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2048.png)

Podemos corrobar que en S3 que se ha creado un nuevo archivo, filtrado, listo para analítica en su enlace de salida:

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2049.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2049.png)

Este archivo, al ser descargado contiene el resultado de nuestra transformación:

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2050.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2050.png)

### Flujo con información de COVID

Se creó un trabajo con AWS Glue Studio

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2051.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2051.png)

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2052.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2052.png)

Hacemos un drop de algunos campos que no me interesan como por ejemplo la pertenencia a un grupo étnico:

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2053.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2053.png)

Seleccionamos cómo queremos que nos salga la base de datos resultante:

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2054.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2054.png)

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2055.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2055.png)

Guardar y correr el Job:

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2056.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2056.png)

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2057.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2057.png)

Corrió exitosamente:

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2058.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2058.png)

Hacemos un query para ver cómo nos quedó la base de datos nueva, utilizando AWS Athena

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2059.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2059.png)

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2060.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2060.png)

# Extra: Analítica con PySpark, Sistema de recomendaciones NETFLIX

## Introducción

Este proyecto tiene como objetivo construir un mecanismo de recomendación de películas dentro de Netflix. El conjunto de datos que utilicé aquí proviene directamente de Netflix y se encuentra en el bucket `02raw/05_PELICULAS/NETFLIX`. 

Consiste en 4 archivos de datos de texto, cada archivo contiene más de 20 millones de filas, es decir, más de películas 4K y clientes 400K. ¡En total, más de 17.000 películas y más de 500.000 clientes!.

Decidimos hacer este ETL offline y no usar glow en primera instancia dada la dificultad de transformar y así hacer que los datos disponibles por netlix sean digeribles en el motor de recomendaciones, que obviamente tiene una estructura fija.

El archivo de entrada tiene la particularidad de que tiene una línea que representa el id de la película y para esta N líneas con el par user_id, Rating

**Ejemplo de datos de input '../row/combined_data_1.txt'**

Dataset 1 shape: (24058263, 2)

- Dataset examples-

user_id  Rating

0              1:     NaN

5000000   2560324     4.0

10000000  2271935     2.0

15000000  1921803     2.0

20000000  1933327     3.0

**¡Movie ID es realmente un lío de importar!.** Realizo mi tarea creando primero una matriz numpy con la longitud correcta, luego agrego toda la matriz como una columna en el marco de datos principal. Esto es el resultado:

*Dataset examples-*

user_id  Rating  movie_id

1         1488844     3.0         1

5000996    501954     2.0       996

10001962   404654     5.0      1962

15002876   886608     2.0      2876

20003825  1193835     2.0      3825

El siguiente paso fue concatenar los 4 dataframes en uno para poder cargarlos en S3 y poder analizar el modelo en pyspark. Para este último creamos un notebook en google colab el cual usó los datos de AWS

## Collaborative filtering

El collaborative filetering se usa comúnmente para **los sistemas de recomendación**. Estas técnicas tienen como objetivo completar las entradas faltantes de una matriz de asociación de usuario-elemento. spark.mllib actualmente admite el collaborative filetering basado en modelos, en el que los usuarios y los productos se describen mediante un pequeño conjunto de factores latentes que se pueden utilizar para predecir las entradas que faltan. spark.mllib usa el algoritmo de mínimos cuadrados alternos (ALS) para aprender estos factores latentes. **[1]**

### Description de el algoritmo

El algoritmo de mínimos cuadrados alternos (ALS) factoriza una matriz R dada en dos factores U y V tal que R≈UTV. La dimensión desconocida de la fila se da como parámetro del algoritmo y se denomina factores latentes. Dado que la factorización matricial se puede utilizar en el contexto de la recomendación, las matrices U y V puede llamarse matriz de usuario y elemento, respectivamente. El ith la columna de la matriz de usuario se denota por ui y  la ith columna de la matriz de elementos es vi. La matriz R se puede llamar la matriz de calificaciones con (R) i, j = ri,j.

Fijando una de las matrices U o V, obtenemos una forma cuadrática que se puede resolver directamente. Se garantiza que la solución del problema modificado reducirá monótonamente la función de costes global. Aplicando este paso alternativamente a las matrices U y V , podemos mejorar iterativamente la factorización de la matriz. **[2]**

### Método

En el siguiente ejemplo, cargamos datos de calificaciones del conjunto de datos de Netflix **[3]** apropiadamente modificado por

ETL , cada fila consta de un usuario, una película, una calificación . Luego entrenamos un modelo ALS y evaluamos el modelo de recomendación midiendo el error cuadrático medio de la predicción de calificación. Dividimos el conjunto de datos en 80% de entrenamiento y 20% de prueba, entrenamos el modelo y evaluamos la predicción, con un error cuadrático medio de alrededor de 2. Luego, hicimos algunas "consultas" al modelo, como "Generar las 10 mejores recomendaciones de películas para cada usuario **[figura A]**"o" Generar las 10 mejores recomendaciones de usuario para un conjunto específico de películas **[figura B]**", etc.

**Figura A.**

+------+--------------------+

|userId| recommendations|

+-------+--------------------+

| 471|[[2363, 6.448765]...|

| 1088|[[2367, 9.536041]...|

| 2122|[[2363, 6.632059]...|

| 2659|[[2481, 8.777299]...|

| 4101|[[3171, 4.6980233...|

| 6336|[[3171, 7.516101]...|

| 7554|[[3277, 7.44603],...|

| 8638|[[2367, 3.661239]...|

| 10817|[[3171, 5.3699374...|

| 14450|[[1378, 6.373323]...|

+-------+--------------------+

**Figura B.**

+-------+--------------------+

|movieId| recommendations|

+-------+--------------------+

| 26|[[2192870, 7.2854...|

| 474|[[1030457, 8.2409...|

| 29|[[1139478, 8.1707...|

+-------+--------------------+

### **Código y desarrollo futuro**

Todo el código empleado en este desarrollo se encuentra en el repositorio GitHub Público:

[ctezna/ARI-movies-2021](https://github.com/ctezna/ARI-movies-2021)

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2061.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2061.png)

![ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2062.png](ST1800%20-%20Trabajo%201%206cfa401608c842d7b03a6701b5b0adf2/Untitled%2062.png)

Notebook del procesamiento con PySpark

En cuanto a desarrollos futuros, podríamos agregar a través de **BOTO3** la posibilidad de leer el conjunto de datos y escribirlo directamente en **S3** y montar un clúster EMR para hacer que todo el flujo o pipeline esté más integrada con el ecosistema de **AWS.**

## Bibliografia

[https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/libs/ml/als.html](https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/libs/ml/als.html) [1]

[https://spark.apache.org/docs/2.2.0/ml-collaborative-filtering.html](https://spark.apache.org/docs/2.2.0/ml-collaborative-filtering.html) [2]

https://www.kaggle.com/netflix-inc/netflix-prize-data/notebooks[3]